{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vibhuarvind/Content-Engine-RAG-for-PDF/blob/main/Content_Engine_PDF_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTH4e8dN8jyb"
      },
      "source": [
        "# **Installing Dependencies**\n",
        "\n",
        "1. **LangChain**: LangChain is a framework for developing applications powered by language models. It provides a collection of components and tools for building end-to-end language applications, including data loaders, text splitters, vector stores, and memory management. In the context of a PDF RAG system, LangChain can be used to load and process PDF documents, split text into chunks, and manage the interaction between the retrieval system and the language model.\n",
        "\n",
        "2. **Torch**: PyTorch is a popular open-source machine learning library based on the Torch library. It provides a wide range of tools and functionalities for building and training neural networks, including support for deep learning, computer vision, and natural language processing. In the context of a PDF RAG system, PyTorch can be used to train or fine-tune a language model, such as BERT or T5, for the specific domain of the PDF documents.\n",
        "\n",
        "3. **Sentence Transformers**: Sentence Transformers is a Python library for generating sentence embeddings using pre-trained transformer models. Sentence embeddings are dense vector representations of sentences that capture their semantic meaning. In the context of a PDF RAG system, Sentence Transformers can be used to generate embeddings for the text chunks extracted from the PDF documents, which can then be stored in a vector database for efficient retrieval.\n",
        "\n",
        "4. **Faiss-CPU**: Faiss is a library for efficient similarity search and clustering of dense vectors. Faiss-CPU is a version of Faiss that runs on the CPU, making it suitable for smaller-scale applications or for running on systems without a GPU. In the context of a PDF RAG system, Faiss-CPU can be used to build a vector index for the sentence embeddings, enabling fast and efficient retrieval of relevant text chunks based on user queries.\n",
        "\n",
        "5. **PyPDF**: PyPDF is a Python library for working with PDF files. It provides functionalities for reading, writing, and manipulating PDF documents, including extracting text and images. In the context of a PDF RAG system, PyPDF can be used to extract text from the PDF documents, which can then be processed and used to build the knowledge base.\n",
        "\n",
        "6. **llama-cpp-python**: llama-cpp-python is a Python library for running the llama.cpp project, which is a C++ implementation of the LLaMA language model. LLaMA is a large-scale language model developed by Meta AI. In the context of a PDF RAG system, llama-cpp-python can be used to run a local instance of the LLaMA language model, which can be used to generate responses based on the retrieved information.\n",
        "\n",
        "7. **Accelerate**: Accelerate is a Python library for accelerating PyTorch models on GPU and TPU. It provides functionalities for distributed training, mixed precision training, and gradient accumulation, which can improve the performance and efficiency of training language models. In the context of a PDF RAG system, Accelerate can be used to optimize the training of the language model on GPU or TPU, if available.\n",
        "\n",
        "8. **Transformers**: Transformers is a Python library for state-of-the-art natural language processing (NLP) models. It provides a wide range of pre-trained models and tools for tasks such as text classification, named entity recognition, and question answering. In the context of a PDF RAG system, Transformers can be used to load and fine-tune pre-trained language models, such as BERT or T5, for the specific domain of the PDF documents.\n",
        "\n",
        "9. **Hugging Face Hub**: The Hugging Face Hub is a platform for sharing and hosting NLP models and datasets. It provides a user-friendly interface for browsing, downloading, and using pre-trained models and datasets. In the context of a PDF RAG system, the Hugging Face Hub can be used to access and download pre-trained language models and embeddings, which can be used to build the knowledge base and generate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggD6SHR894ne",
        "outputId": "3139f3d4-0dd5-4861-8f4a-c898a60e3510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/975.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/975.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/975.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m860.2/975.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
            "  Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 orjson-3.10.5\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.40.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=3748167 sha256=baff4cdd18da74243cf7ececd850afd84439fae2b2b9aa7884d0624e51763f41\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install llama-cpp-python\n",
        "!pip -q install accelerate\n",
        "!pip install transformers\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tau1XxORFDGk",
        "outputId": "a39ed9dd-9b8f-4b87-9ed6-8495a35dd994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain-community) (24.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYoxAToUGzZD"
      },
      "source": [
        "# Mistral_7B_Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I9-9J71Z-lbC"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-sSwP-JBLrx"
      },
      "source": [
        "# Loading pdf content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y6tFzitaE53Y"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFDirectoryLoader('documents/')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAgxoMaBGW7-",
        "outputId": "a55f2d9e-8c6e-49f0-a1a2-76df00a6c9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 0, 'page_label': '1'}, page_content='D i s e a s e C l a s s i f i c a t i o n i n P a t i e n t X - R a y s u s i n g D e e p L e a r n i n g A p p r o a c h e s \\n S r i n i v a s N a t a r a j a n SCAI Arizona State University T empe AZ US snatar28@asu.edu \\nS a i t e j a A l a p a r t h i SCAI Arizona State University T empe AZ US salapart@asu.edu \\nT a n u s h i A h u j a SCAI Arizona State University T empe AZ US tahuja4@asu.edu \\nD a r s h a n G o v i n d a r a j SCAI Arizona State University T empe AZ US dgovind5@asu.edu \\nA B S T R A C T The interpretation of X-rays is critical for identifying various medical conditions and their precursors. In this project, we explore the potential of using computer systems to aid in the diagnosis process using various deep learning approaches for ef ficient disease localization in patient X-rays. Specifically , we use standard boosting techniques, deep learning architectures, specialized architectures for the medical field, and transformer -based approaches. Our results demonstrate the ef fectiveness of deep learning approaches for disease localization in X-rays, with implications for improving the accuracy and ef ficiency of medical diagnoses. \\n1. Intr oduction \\n Medical imaging has revolutionized healthcare by enabling the early detection and diagnosis of a wide range of diseases. However , the increasing number of medical images generated has resulted in a growing demand for skilled radiologists and technicians, which has become a bottleneck in the healthcare system. This has highlighted the need for computer -aided diagnosis (CAD) systems to help clinicians make accurate diagnoses in a timely and cost-ef fective manner . Deep learning approaches have shown great potential in the field of medical imaging, particularly in the localization of diseases in X-rays. By using various preprocessing techniques and classification algorithms, deep learning models can ef fectively identify and localize diseases in patient X-rays with a high degree of accuracy . This can have significant implications for improving the ef ficiency and accuracy of medical diagnoses, potentially leading to earlier detection and treatment of diseases. In this project, we aim to explore the potential of deep learning approaches for disease localization in patient X-rays, with a focus on four dif ferent approaches: standard boosting techniques, generalized deep learning architectures, and \\n specialized architectures for the medical field, and transformer -based approaches. Through this investigation, we hope to demonstrate the ef fectiveness of deep learning models in aiding clinicians in the diagnosis of medical conditions using X-rays. \\n2. Related W ork In the process of tackling this medical classification and localization problem, we encountered a few major types of approaches to create an accurate model. The main approach and a theme that is seen through most work done in this field involves the use of pre-trained architectures like Residual neural networks (ResNet38/50) [3]. These approaches are an industry standard due to the lack of lar ge amounts of data to tackle medical problems, making transfer learning a more viable method. Most papers we came across either simply apply this model to approach this task or train very few layers at the end of the network based on the new data. On the other hand, we unfreeze a lot of layers of the models and add additional layers at the end before training, helping finetune the model. \\n Deviating a little further from standard CNNs, we see work done where conventional machine learning classifiers are combined with the architecture to provide an alternative approach [4]. This includes using SVMs and linear classifiers as substitutes for the final sigmoid layer of deep learning models. This approach does show some success but is lacking in the use of global context available in the image and in the speed of computation. This is where a variety of dif ferent models have been developed, making a medical system that is viable in real time. This also cuts down the computation costs associated with image classification. One such approach is the use of lightweight models like the MobileNetv2 [6] that are designed to run on smaller , less powerful systems. These types of models achieve a similar level of performance as the initial work [3],[5] mentioned but are far faster in run time and are less resource intensive. The other way to approach this is by \\n 1 '), Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 1, 'page_label': '2'}, page_content='using modified architectures incorporating atrous convolutions as in [10]. This type of work on ef ficient systems was more important back when systems were less powerful but current systems have all but caught up in terms of prediction speed. Even lar ger models like ResNet 50 have pretty fast prediction times. \\n W ith the advancement of computing technology , it has become feasible to use a transformer based system consisting of two main components: encoder networks to extract information from the data and decoder systems that break down this encoding and classify the output. W e see a growing usage of partial context systems such as residual attention networks [8] that use attention mechanisms to both category wise [8] and globally [9] to capture contextual information to be used for predictions. This also leads to the possibilities of full transformer networks for image classification tasks where originally they were limited to NLP problems. This trend was started with Microsoft’ s Swin transformer architecture which was created to tackle multiple modalities and not just NLP tasks. This can be seen in systems like RA TCHET [5] where two separate architectures are combined (CNN+T ransformer in this example) to of fer better results than traditional methods. This is an approach that we aim to explore further in our project. \\n W e also explore more novel approaches, like incorporation of patient metadata to augment image based prediction systems. This is because papers found a positive correlation between patient data like age, blood type and gender . But current work on this has yet to show better results than traditional image systems. A sample architecture has been mentioned below: \\n Fig. 1 : Combined approach architecture \\n Finally , it is important to address any real world side-ef fect our project may have, such as intrinsic biases. This is why we also explored studies on existing datasets [7] to analyze how data can be skewed based on patient information and factors. This is essential to create a system that can help address any societal and demographic imbalances and make it overall, more objective. \\n3. Data 3 . 1 . E x p l o r a t o r y D a t a A n a l y s i s For this problem, we used the Chest X-ray 14 dataset created by the National Institute of Health. This is a dataset that consists of nearly 1 10,000 X-ray images with two main parts of metadata. \\n Each x-ray can consist of multiple health conditions from the following classes: \\'Atelectasis\\', \\'Consolidation\\', \\'Infiltration\\', \\'Pneumothorax\\', \\'Edema\\', \\'Emphysema\\', \\'Fibrosis\\', \\'Ef fusion\\', \\'Pneumonia\\', \\'Pleural Thickening\\', \\'Cardiomegaly\\', \\'Nodule\\', \\'Mass\\', \\'Hernia\\' and ‘No Findings’. It also consists of boundaries for each of these conditions for localization problems. \\n Fig. 2: Class distribution of the chest conditions \\n V isualizing the class distribution in the data, we can see that there is a significant imbalance between the majority and minority classes. There are three possible approaches forward: first is to ignore this problem and go ahead with model creation, although this may skew the results. Second, weigh each class inversely according to its dif ficulty to detect and select a certain number of instances from each class. The third is to reduce the number of overall instances by undersampling the data. \\n3 . 2 . D a t a P r o c e s s i n g W e reduce the overall number of instances by choosing only 40,000 X-rays, making sure to include all available instances of the minority classes. This proved to be the most ef fective method. The next step is to process our images and labels by defining custom data loaders. Each image is resized to a 128x128 dimension, grayscaled, and converted to tensors. More experimentation on the varying sizes of the image along with augmentations will be performed in the future and will be a part of our ablation study . W e form labels for each image by one-hot encoding a vector of size 14, with \"1\"’ representing the presence of the disease and \"0\"’ representing its absence. \\n The data is then split in the ratio 80-10-10 for the training, testing, and validation sets, respectively . This is important because we use validation loss as a measure for our early stopping callback. This helps us prevent overfitting. \\n 2 \\n'), Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 2, 'page_label': '3'}, page_content='Fig. 3: Representative samples of some classes \\n4 . M e t h o d s The work done will be divided into two main sections: experimentation on the preprocessing performed on the image which involves various input sizes, channel sizes, data balancing techniques and augmentation techniques which will form our ablation study and dif ferent approaches to deep learning approaches that involve standard convolutional neural networks[3], combined approaches of neural networks and traditional classification approaches like Gaussian Mixture Models (GMMs) or K Means [4], modified approaches that focus on the speed of prediction by leveraging shallower networks and techniques like atrous layers [6,10] and the stretch goal for our project will be the use of the next generation transformer models that aim to bridge the gap between the text and image modalities. \\n4 . 1 . T r a d i t i o n a l D e e p L e a r n i n g In this section, we will explore traditional deep learning models that use convolutions and their variations. This will include: ResNets and MobileNet. W e modify the architecture of these models to include an average pooling layer . Due to overfitting concerns, we added dropout layers as these are relatively dense architectures. \\n Fig. 4: ResNet50 Model Summary \\n Fig. 5: Mobile Net Model Summary \\n4 . 2 . M i x e d L e a r n i n g This section will look into the use of deep learning networks to generate a set of features that will be used by more traditional classifiers. For each image, information is extracted by a convolutional neural network, and the dimensionality is reduced using PCA before being passed as features to models like Random Forest, GMMs, and K Means for classification. This approach is something suggested in previous research [4], but we will expand upon the SVM approach taken. \\n4 . 3 . T r a n s f o r m e r M o d e l s This section will be part of our stretch goals, where we will try to expand upon recent advances in transformer technology , converting them from an architecture initially suited for only natural language tasks and bridge the modality to image tasks as well. For this purpose, we will be basing our work on the RA TCHET transformer system [5] and Microsoft’ s new Swin architecture, which uses sliding windows to retain attention in images. For practical purposes, we will use pre-trained network encoder weights as the default ‘ImageNet’ weights. The model consists of three overall sizes, Swin-T (0.25x size), Swin-S (0.5x size), and Swin-L (2x size). Increasing the size of the model comes at the risk of overfitting due to the amount of data present for the minority classes, so this will have to be monitored carefully . \\n Fig. 6: Swin T ransformer Architecture vs. T raditional V ision T ransformers \\n 3 \\n'), Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 3, 'page_label': '4'}, page_content='5 . R e s u l t s Our results will consist of benchmarking standard models and contrasting them with the improved model we have constructed. W e will measure the AUC and F-score performances of the models and aim to create a novel system that can achieve a near state of the art (SoT A) performance. W e modified the existing ResNet and MobileNet models with additional Global average pooling layers, Dropouts and a final output layer of 14 classes. They were trained with the ‘Adam’ optimizer with a learning rate of 0.01 and a Softmax activation layer as the output. \\n Fig. 7: AUC scores for the modified ResNet50 model \\n Fig. 8: AUC scores for the modified MobileNet model \\n Disease / Model \\n Mobile Net \\n ResNet 50 \\n Dense Net \\n Swin - L \\n Atelectasis 0.75 0.76 0.834 0.84 \\n Cardiomegaly 0.85 0.83 0.913 0.93 \\n Consolidation 0.70 0.68 0.818 0.87 \\n Edema 0.84 0.87 0.914 0.92 \\n Ef fusion 0.82 0.83 0.905 0.82 \\n Emphysema 0.87 0.90 0.936 0.93 \\n Fibrosis 0.79 0.79 0.786 0.94 \\n Hernia 0.67 0.70 0.665 0.85 \\n Infiltration 0.76 0.76 0.886 0.71 \\n Mass 0.68 0.76 0.790 0.89 \\n Nodule 0.72 0.71 0.813 0.87 \\n Pleural Thickening \\n 0.66 0.66 0.807 0.90 \\n Pneumonia 0.83 0.88 0.906 0.93 \\n Pneumothorax \\n 0.75 0.76 0.834 0.85 \\n T able 1: Comparison of AUC scores of model \\n W ith the complexity and quantity of data available in the NIH Chest X-ray dataset, we can see from the results that having a deeper network can improve results. W e compare three types of neural networks and the best performing transformer model. The MobileNet model represents a lightweight, shallow model that focuses on fast processing times over performance. The Resnet 50 model is a standard for image analysis tasks but has been fine tuned for this particular task. Coupled with dropout layers, it prevents overfitting. The Densenet 121 is the deepest model we use, and modifications to its structure help prevent overfitting, which is an inherent risk with its depth. This is the best performing model on our test set. W e also experimented with various batch sizes, color channels, image sizes, and learning rates. W e tested their ef fectiveness based on their conver gence rates and the loss over epochs. \\n 4 \\n'), Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 4, 'page_label': '5'}, page_content='5 . 1 . I m a g e S i z e Image size is an important factor , as lar ger images allow us to extract more information, but this comes at the cost of computational and memory overhead. This is why we conducted a study of the various sizes of images to feed to the model and measured how the model performed based on its conver gence and loss. W e use standard image sizes ranging from 1024x1024 to 64x64. W e also include a more irregular 224x224 as this is the standard window size for vision transformer models. \\n Fig. 9: Loss over epochs for dif ferent image sizes \\n W e see that the best performing sizes are 512x512 and 256x256, with both of them having pretty similar results. W e went forward with 256x256 images as it saves on the memory and time we need to compile and train our models. For the Swin T ransformers, we are limited to a size of 224x224 due to pretrained weight constraints. \\n5 . 2 . C o l o r C h a n n e l s The two main color modes we have available are the standard RGB and a grayscaled version of the image dataset. While normal image processing tasks preferably use grayscaled images to scale down on computational cost, medical scans are more sensitive to changes in intensity . This can be seen in CT scan images, where the intensity measured in Hounsfield units is of great significance. \\n Image Size/ Color Channel \\n 224x224 256x256 \\n RGB 0.340 0.3385 \\n Grayscaled 0.33 0.33823 T able 2: Comparison of model losses \\n Although the loss dif ference is miniscule, we opt to use RGB images for our models, as this also enables us to use pretrained networks like ResNet50 on the ImageNet data. \\n6 . D i s c u s s i o n W e can see from the results why convolutional neural networks have been the de facto standard in image analysis for a good reason. Convolutional neural networks pre-trained on existing data prove to be the most accurate and reliable method. But we also find that V ision based transformers have improved leaps and bounds from their inception, with the lar ger versions of the Swin transformer . (Swin-L) proving to have SoT A results. It is also clear that these transformer models are far more sensitive to tuning of hyperparameters like window size, patch size, and model depth. This can be seen in the dif ference between the various sizes of Swin transformers, from the tiny Swin-L (0.25x Swin-B) to the enormous Swin-L (2x W in-B). But they also of fer near SoT A performance, with an average AUC of 0.81, approaching the best results to date. While the inclusion of metadata involving a patient’ s records can seem intuitively productive, current systems have not been able to leverage them. This presents a future avenue for exploration to improve systems to combat serious medical issues. \\n7 . C o n c l u s i o n This project emphasizes the potential of machine learning systems and their potential to augment medical professionals in the diagnosis of serious throasic issues. While current datasets are highly skewed, resembling anomaly detection tasks, improvements in the or ganization of data and the availability of medical information have greatly increased the feasibility of computer aided diagnosis. The use of transformer based models that can leverage context from various parts of the image has revolutionized the ability to accurately predict medical conditions. In conclusion, we have created and fine tuned both models that have been the standard for a decade and contrasted them to more recent developments in deep learning to address a pressing issue in current society . W e created a model capable of identifying and localizing serious thoracic issues with an accuracy rivaling state of the art current systems. In the future, we can expand this system to include patient data and previous medical conditions to better use medical history in the diagnosis of diseases, as doctors do. This system can also be augmented to use better medical techniques used by professionals to improve its performance and learn from its mistakes. \\n 5 \\n'), Document(metadata={'source': 'documents\\\\DM Final Project Report.pdf', 'page': 5, 'page_label': '6'}, page_content='R E F E R E N C E S [ 1 ] G i e ł c z y k A , M a r c i n i a k A , T a r c z e w s k a M , L u t o w s k i Z ( 2 0 2 2 ) P r e - p r o c e s s i n g m e t h o d s i n c h e s t X - r a y i m a g e c l a s s i f i c a t i o n . P L o S O N E 1 7 ( 4 ) : e 0 2 6 5 9 4 9 . h t t p s : / / d o i . o r g / 1 0 . 1 3 7 1 / j o u r n a l . p o n e . 0 2 6 5 9 4 9 [ 2 ] I v o M . B a l t r u s c h a t , H a n n e s N i c k i s c h , M i c h a e l G r a s s , T o b i a s K n o p p , a n d A x e l S a a l b a c h . 2 0 1 9 . C o m p a r i s o n o f d e e p l e a r n i n g a p p r o a c h e s f o r m u l t i - l a b e l c h e s t X - r a y c l a s s i f i c a t i o n . S c i e n t i f i c R e p o r t s 9 , 1 ( 2 0 1 9 ) . D O I : h t t p : / / d x . d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 5 9 8 - 0 1 9 - 4 2 2 9 4 - 8 [ 3 ] I . M . B a l t r u s c h a t , H . N i c k i s c h , M . G r a s s , T . K n o p p , a n d A . S a a l b a c h , “ C o m p a r i s o n o f d e e p l e a r n i n g a p p r o a c h e s f o r m u l t i - l a b e l c h e s t X - r a y c l a s s i f i c a t i o n , ” N a t u r e N e w s , 2 3 - A p r - 2 0 1 9 . h t t p s : / / w w w . n a t u r e . c o m / a r t i c l e s / s 4 1 5 9 8 - 0 1 9 - 4 2 2 9 4 - 8 . [ 4 ] I . A l l a o u z i a n d M . B e n A h m e d , \" A N o v e l A p p r o a c h f o r M u l t i - L a b e l C h e s t X - R a y C l a s s i f i c a t i o n o f C o m m o n T h o r a x D i s e a s e s , \" i n I E E E A c c e s s , v o l . 7 , p p . 6 4 2 7 9 - 6 4 2 8 8 , 2 0 1 9 , d o i : 1 0 . 1 1 0 9 / A C C E S S . 2 0 1 9 . 2 9 1 6 8 4 9 . h t t p s : / / i e e e x p l o r e . i e e e . o r g / a b s t r a c t / d o c u m e n t / 8 7 1 9 9 0 4 [ 5 ] ￼ B . H o u , G . K a i s s i s , R . S u m m e r s , a n d B . K a i n z , “ R a t c h e t : M e d i c a l T r a n s f o r m e r f o r c h e s t X - r a y d i a g n o s i s a n d r e p o r t i n g , ” a r X i v . o r g , 1 5 - S e p - 2 0 2 1 h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 1 0 7 . 0 2 1 0 4 . [ 6 ] A . S o u i d , N . S a k l i , a n d H . S a k l i , “ C l a s s i f i c a t i o n a n d p r e d i c t i o n s o f l u n g d i s e a s e s f r o m c h e s t X - r a y s u s i n g M o b i l e N e t V 2 , ” M D P I , 1 8 - M a r - 2 0 2 1 . h t t p s : / / w w w . m d p i . c o m / 2 0 7 6 - 3 4 1 7 / 1 1 / 6 / 2 7 5 1 . [ 7 ] L . S e y y e d - K a l a n t a r i , G . L i u , M . M c D e r m o t t , I . Y . C h e n , a n d M . G h a s s e m i , “ C h e x c l u s i o n : F a i r n e s s g a p s i n d e e p c h e s t X - r a y c l a s s i f i e r s , ” a r X i v . o r g , 1 6 - O c t - 2 0 2 0 . h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 0 0 3 . 0 0 8 2 7 . h t t p s : / / a r x i v . o r g / a b s / 2 0 0 3 . 0 0 8 2 7 v 2 [ 8 ] Q . G u a n a n d Y . H u a n g , “ M u l t i - l a b e l c h e s t X - r a y i m a g e c l a s s i f i c a t i o n v i a c a t e g o r y - w i s e r e s i d u a l a t t e n t i o n l e a r n i n g , ” P a t t e r n R e c o g n i t i o n L e t t e r s , v o l . 1 3 0 , p p . 2 5 9 – 2 6 6 , 2 0 2 0 . h t t p s : / / w w w . s c i e n c e d i r e c t . c o m / s c i e n c e / a r t i c l e / a b s / p i i / S 0 1 6 7 8 6 5 5 1 8 3 0 8 5 5 9 [ 9 ] Y u , K . , G h o s h , S . , L i u , Z . , D e i b l e , C . , a n d B a t m a n g h e l i c h , K . “ A n a t o m y - G u i d e d W e a k l y - S u p e r v i s e d A b n o r m a l i t y L o c a l i z a t i o n i n C h e s t X - r a y s ” , a r X i v . o r g , 2 5 - S e p - 2 0 2 2 , h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 2 0 6 . 1 2 7 0 4 , h t t p s : / / a r x i v . o r g / a b s / 2 2 0 6 . 1 2 7 0 4 [ 1 0 ] Ö . Ö z d e m i r a n d E . B . S ö n m e z , \" W e i g h t e d C r o s s - E n t r o p y f o r U n b a l a n c e d D a t a w i t h A p p l i c a t i o n o n C O V I D X - r a y i m a g e s , \" 2 0 2 0 I n n o v a t i o n s i n I n t e l l i g e n t S y s t e m s a n d A p p l i c a t i o n s C o n f e r e n c e ( A S Y U ) , I s t a n b u l , T u r k e y , 2 0 2 0 , p p . 1 - 6 , d o i : 1 0 . 1 1 0 9 / A S Y U 5 0 7 1 7 . 2 0 2 0 . 9 2 5 9 8 4 8 . [ 1 1 ] L i u , Z . , L i n , Y . , C a o , Y . , H u , H . , W e i , Y . , Z h a n g , Z . , L i n , S . , & G u o , B . ( 2 0 2 1 , A u g u s t 1 7 ) . S w i n T r a n s f o r m e r : H i e r a r c h i c a l v i s i o n t r a n s f o r m e r u s i n g s h i f t e d w i n d o w s . a r X i v . o r g . R e t r i e v e d A p r i l 1 3 , 2 0 2 3 , f r o m h t t p s : / / a r x i v . o r g / a b s / 2 1 0 3 . 1 4 0 3 0 \\n6 '), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 0, 'page_label': '1'}, page_content=' \\n \\n1. Project 1 (Colonoscopy) \\n1.1. Colonoscopy Video Quality Assessment \\n(Classification) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can assess frames of a colonoscopy \\nprocedure and determine its quality. This is important as \\ngood quality frames are needed for the localization and \\nsegmentation of polyps. \\n \\n1.1.1 Dataset Exploration \\nThe dataset used is from the University of Toronto in \\n2015. It consists of  1,062 images with an equal class \\ndistribution. The sample images of the clear and burry \\nimages of the colonoscopy video are given below. \\n \\n \\nFig 1: Class Images \\n \\nThe images are of the shape 480x712x3 which is larger \\nthan we need. The preprocessing I do is explained below. \\n \\n1.1.2 Data Processing \\nThis situation can be seen as a binary classification \\nproblem where each frame is classified as ‘clear’ (label 0) \\nor ‘blurry’ (label 1). To process the images, I first create a \\ncustom python class to represent it. Each instance stores \\nthe label as well as a processed image using OpenCV and \\nPyVison’s transforms. The images are resized to a \\n256x379 dimension and gray scaled to improv e \\nperformance and converted to Tensors.  This data is split \\nas 80-20 for the training and testing sets \\n \\n \\nFig 2: Processed Image \\n \\n1.1.3 Model \\nThe model I chose  for this problem is the ResNet -18. I \\nchose this over larger models as when running video \\ncomputation, speed is also a critical factor. It is modified \\nfor this resolution with two output nodes. In addition, I \\nadded dropout layers  (p=0.2) in between the den se layers \\nto prevent any overfitting. \\n \\n \\nFig 3: Model Summary \\n \\nThe Adam optimizer is used with a learning rate of 0.01. \\nThe loss function used is the standard cross entropy loss. \\n \\n1.1.4 Results \\nThe model is trained for 23 epochs. Any more resulted in \\noverfitting. The training is accelerated with the help of a \\nGPU.  \\n \\n \\nFig 4: Model training \\n \\nOf the multiple runs done, the best result achieved is an \\naccuracy of 91.080% and an F-score of 0.911. As possible \\nimprovements for this problem, we could finetune the \\n \\nClassification Report \\n \\n \\nSrinivas Natarajan \\nArizona State University \\nsnatar28@asu.edu \\n '), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 1, 'page_label': '2'}, page_content=' \\n \\nmodel a little more by altering the learning rate over time. \\n \\n \\nFig 5: Classification report of the model \\n \\nThe overall metrics of the model can be seen in the \\nincluded graph with an average AUC score of 0.862. The \\nclass predictions are visualized through a heatmap as \\nshown below. \\n \\n \\n \\nFig 6: Confusion Matrix of the predictions \\n \\n \\n1.2. Polyp Segmentation (Segmentation) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can accurately segment the boundaries \\nof polyps in the gastrointestinal tract (GI) \\n \\n1.2.1 Dataset Exploration \\nThe Clinic -CVC dataset is used for this exercise. It \\nconsists of 612 images of frames from a colonoscopy \\nprocedure, each consisting of an ima ge an image and a \\nmask indicating the boundaries of the polyp. The \\nrepresentation can be seen below.  \\n \\n \\nFig 7: Data set images provided \\n \\n1.2.2 Data Processing \\nThe data is resized to a 288x384 dimension and \\nconverted to gray scale using OpenCV.  Due to the small  \\nsize of the data set, I decided to augment the training data \\nusing Horizontal flips, vertical flips and zooms.  \\n \\n \\nFig 8: Processed masks \\n \\nI create a custom python class to represent it. Each \\ninstance stores the label as well as a processed image \\nusing Op enCV and PyVison’s transforms. Augmentation \\nis handled with PyTorch’s A lbumentations subpackage. \\nThis data is then split as 80 -10-10 for the training, \\nvalidation and testing sets. \\n \\n1.2.3 Model \\nI chose the UNet, UNet++ and the DeepLabv3 models for \\nthis task. Firs t for the UNet model, I used the ResNet50 \\nstructure for the encoder architecture with the ImageNet \\nweights. I used Dice Loss for the loss function and IoU \\nfor the score. The Adam Optimizer is used with a learning \\nrate of 0.0001 and a learning rate schedule r to reduce the \\nlearning rate once it plateaus.  \\n \\n \\nFig 9: UNet Model Architecture \\n \\nFor the UNet ++ model and DeepLabv3, I use similar \\nparameters to make sure I can understand the difference \\nbetween these models. \\n \\nFig 10: DeepLabv3 Architecture \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 2, 'page_label': '3'}, page_content=' \\n \\n \\n1.2.4 Results \\nThe models are each trained for 10 epochs with a batch \\nsize of 16. Any more resulted in overfitting. The training \\nis accelerated with the help of a GPU.  \\n \\n \\nFig 11: UNet Model training \\n \\nOf the runs done, the best result tha t the UNet model \\nachieved is an IoU score of 0.982 and a Dice loss of \\n0.044. The variations of the IoU score and Dice loss can \\nbe seen below. \\n \\n \\nFig 12: IoU scores of the UNet model \\n \\n \\nFig 13: Dice loss of the UNet model \\n \\nFor the UNet++ model, a similar ap proach was taken and \\nthe training was halted after 9 epochs to prevent \\noverfitting. \\n \\n \\nFig 14: UNet++ Model training \\n \\nThe best results achieved by the UNet++ model is an IoU \\nscore of 0.9756 and a Dice loss of 0.1250. The variations \\nof both can be seen below. \\n \\n \\nFig 15: IoU scores of the UNet++ model \\n \\n \\nFig 16: Dice loss of the UNet++ model \\n \\nFor the DeepLabV3 model, the best results achieved were \\nan IoU score of 0.9716 and a dice loss of 0.05. \\n \\n \\nFig 17: IoU scores of the DeepLabv3 model \\n \\nThe best performing model is the UNet++ model \\naccording to my work. \\n \\n1.3. Polyp Bounding Boxes (Localization) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can assess frames of a colonoscopy \\nprocedure and locate polyps in the GI. This is important \\nas in aiding doctors in the identification process. \\n \\n1.3.1 Dataset Exploration \\nThe dataset used is from the University of Toronto in \\n2015. It consists of 1,062 images with an equal class \\ndistribution. The samp le images of the clear and burry \\nimages of the colonoscopy video are given below. The \\nimages are of the shape 480x712x3 which is larger than \\nwe need. The preprocessing I do is explained below. \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 3, 'page_label': '4'}, page_content=\" \\n \\n1.3.2 Data Processing \\nThis situation can be seen as a localization task with \\ncoordinates for the bounding box for each frame. Each \\ninstance stores the label as well as a processed image \\nusing OpenCV and PyVison’s transforms. The images are \\nresized to a 256x379 dimension and gray scaled to \\nimprove performance and converted to Tensors. This data \\nis split as 80-20 for the training and testing sets \\n \\n \\n \\nFig 18: Processed Image \\n \\n1.3.3 Model \\nThe model I chose for this problem is the YOLOv5. I \\nchose this as it is one of the best approaches to the \\nlocalization tasks. Leverag ing transfer learning, this \\nmodel is already pretrained with weights, greatly speeding \\nup our task. \\n \\nThe Adam optimizer is used with a learning rate of 0.01. \\nThe loss function used is the standard cross entropy loss. \\n \\n1.3.4 Results \\nThe model is trained for 23 epochs. Any more resulted in \\noverfitting. The training is accelerated with the help of a \\nGPU.  \\n \\n \\nFig 19: Model training \\n \\n \\nFig 20: Model prediction on the image \\n \\n \\nFig 21: Training loss \\n \\nIn the end, the model achieves an IoU score of 0.78 when \\ntested against the ground truth bounding boxes. \\n \\n \\n2. Chest X-rays \\n2.1. Chest X-ray Classification (Classification) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can analyze chest X -rays to identify \\nthe presence of thoracic diseases in a patient.  \\n \\n2.1.1 Dataset Exploration \\nThe dataset used is the NIH Chest X -ray dataset \\nconsisting of 112,120 X-ray instances. It consists of the \\nfollowing classes: 'Atelectasis', 'Consolidation', \\n'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', \\n'Fibrosis', 'Effus ion', 'Pneumonia', 'Pleural Thickening', \\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia' and ‘No \\nFindings’.  We first look into the distribution of classes to \\nmake sure there isn’t any major imbalances. \\n \\n \\nFig 22: Class distribution of the diseases \\n \\nAs can be seen, the classes are skewed with diseases \\nlike ‘Hernia’ having little too few instances. To balance \\nthe data, we assign weights to each class for redistribution \\nand choose 40,000 X -rays that do not have any missing \\ndata. \\n\"), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 4, 'page_label': '5'}, page_content=' \\n \\n \\n \\nFig 23: Redistributed Class frequencies \\n \\n \\n2.1.2 Data Processing \\nThis situation can be seen as a multi-label classification \\nconsisting of 15 classes. The first step is to create a target \\nvector of all the classes for each instance in the data set. \\nThis is done in a manner similar to one hot encoding \\nwhere we have an array of length 15 consisting of 1s \\nindicating the presence of a disease and 0s indicating the \\nabsence. \\n \\n \\nFig 24: Sample images of classes \\n \\nWe use a 75 -15-10 split on the dataset for the training, \\nvalidation and testing se ts. This means that we have \\n30000, 6000 and 4000 instances respectively. The training \\ndata is also augmented using height shift ranges, width \\nshift ranges, zooms and horizontal flips. This will help us \\ngain more cases of classes that are scarce in samples.  The \\nimages are resized to 128x128 and gray scaled to aid in \\ncomputation \\n \\n2.1.3 Model \\nTwo models are compared in this exercise: the ResNet50 \\nand Mobile Net. Both of the base models are initialized \\nand layers of Global average pooling, dense layers and \\ndropouts a re added before the final output layer with a \\nsigmoid activation function.  \\n \\n \\nFig 25: ResNet50 Model Summary \\n \\nThe Adam optimizer is used with a learning rate of 0.01. \\nThe loss function used is the standard cross entropy loss. \\n \\n \\nFig 26: Mobile Net Model Summary \\n \\n \\n2.1.4 Results \\nThe model is trained for 5 epochs with 100 steps per \\nepoch. Any more resulted in overfitting. The training is \\naccelerated with the help of a NVIDIA P100 GPU.  \\n \\n \\nFig 27: Model training \\n \\nMultiple runs were done and the AU C scores were \\nrecorded for both models. They are detailed in the table \\nand graphs below. \\n \\n \\n \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 5, 'page_label': '6'}, page_content=' \\n \\nDisease / Model Mobile Net ResNet50 \\nAtelectasis 0.75 0.834 \\nCardiomegaly 0.85 0.913 \\nConsolidation 0.70 0.818 \\nEdema 0.84 0.914 \\nEffusion 0.82 0.905 \\nEmphysema 0.87 0.936 \\nFibrosis 0.79 0.786 \\nInfiltration 0.67 0.665 \\nMass 0.76 0.886 \\nNodule 0.68 0.790 \\nPleural Thickening 0.72 0.813 \\nPneumonia 0.66 0.807 \\nPneumothorax 0.83 0.906 \\nTable 1: AUC scores of the models \\n \\n \\nFig 28: ROC curve for the ResNet50 model \\n \\n \\nFig 29: ROC curve for the Mobile Net model \\n \\nWe can see that Resnet is clearly the best performing \\nmodel as seen from the accuracy and AUC scores for each \\nclass, achieving an average on 0.81.  Further \\nimprovements can be made with equal balancing of \\nclasses, implementing architectures like the UNet which \\nare designed for medical classification tasks and by \\npossibly including patient data as a second set of \\nparameters for the decision. \\n \\n2.2. Lungs, Heart and Clavicles (Segmentation) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can analyze chest X -rays to accurately \\nsegment the Lungs, Heart and clavicles. \\n \\n2.2.1 Dataset Exploration \\nData used is from the JSRT database consisting of 154 \\nX-ray images of resol ution 2048x2048. These are \\naccompanied by individual masks for each of the 5 \\nclasses mentioned above, all stored in their separate \\nfolders. \\n \\n \\nFig 30.1: Representation of X-ray and heart mask \\n \\n \\nFig 30.2: Representation of clavicle masks \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 6, 'page_label': '7'}, page_content=' \\n \\n \\nFig 30.3: Representation of lung masks \\n \\n \\n2.2.2 Data Processing \\nThis is a segmentation task where we have a limited \\namount of data. The raw image size is too big to fit the \\nbatch in memory and hence need to be resized. A data \\nclass is created mapping each image with its r espective 5 \\nmasks. Each image is resized to 256x26, gray scaled and \\nconverted to tensors. It is then split in an 80 -20% ratio for \\ntraining and testing. Due to the lack of data, I opted to \\nforgo the validation set. This data is then loaded using \\nPyTorch Data Loaders and is displayed below. \\n \\n \\nFig 31.1: Processed Image and heart mask \\n \\nWe use a 75 -15-10 split on the dataset for the training, \\nvalidation and testing sets. This means that we have \\n30000, 6000 and 4000 instances respectively. The training \\ndata is also augmented using height shift ranges, width \\nshift ranges, zooms and horizontal flips. This will help us \\ngain more cases of classes that are scarce in samples. The \\nimages are resized to 128x128 and gray scaled to aid in \\ncomputation \\n \\n  \\nFig 31.2: Processed clavicle masks \\n \\n  \\nFig 31.3: Processed lung masks \\n \\n2.2.3 Model \\nFor the segmentation process, I used the UNet \\narchitecture. To compare the performance of the models \\ntrained from scratch versus a pretrained network, I used \\nthe ImageNet weights and the ResNet18 encoder. \\n \\nIn the selection for loss function, I use Dice Loss as we \\nare handling a segmentation task. The IoU metric is used \\nto determine the score after every epoch. The Adam \\noptimizer is used with an initial learning rate of 0.0003 \\nThe learning rate is  also reduced when it plateaus using \\nthe ‘Step learning Rate’ function by a factor of 0.5 after 3 \\niterations of plateauing. The model is run for 25 epochs.   \\n \\n \\nFig 32: Model Training \\n \\nThe next section will detail the collected data from 10 \\nruns of the model from scratch and 10 more on the \\npretrained version. \\n \\n \\n2.2.4 Results \\n \\nClass Pre- Trained Scratch \\nHeart 0.9544 0.6595 \\nLeft Clavicle 0.7905 0.5668 \\nRight Clavicle 0.8795 0.5377 \\nLeft Lung 0.9748 0.9465 \\nRight Lung 0.9692 0.9568 \\nTable 2 Averaged IoU scores over 10 runs \\n \\nTo see if there is significant statistical difference between \\nthe two results, I set the following hypothesis: \\n1. Null Hypothesis (𝐻0): There is no significant \\ndifference between the results \\n2. Alternate Hypothesis (𝐻1): There is significant \\ndifference between the results \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 7, 'page_label': '8'}, page_content=' \\n \\nI do a two-tail student t -test to test the hypotheses  I put \\nforth.  \\n \\n \\nFig 33: Student t-test values \\n \\nGiven that the p -values does not exceed the 95% \\nthreshold of 0.10 for the  two-tail test, I reject the null \\nhypothesis and conclude that using the pretrained weights \\ngives a significant difference. This also logically makes \\nsense as the model is trained on extremely large datasets \\nwhich generalize very well rather than just on a  single \\ndataset. This helps the model learn more than it ever \\ncould learning purely from the JSRT dataset. \\n \\n2.3. Chest Disease Localization (Localization) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model based that can localize the presenc e of \\nlung diseases from an x-ray image.  \\n \\n2.3.1 Dataset Exploration \\nData used is from the VinDr CXR database consisting \\nof 3000 X -ray images of varying resolutions. These are \\naccompanied by multilabel class strings for each of the 14 \\nclasses. \\n \\n \\n \\nFig 34: Representation of X-ray and heart mask \\n \\nIt consists of the following classes: \"Aortic \\nenlargement\", \"Atelectasis\", \"Calcification\", \\n\"Cardiomegaly\", \"Consolidation\", \"ILD\", \"Infiltration\", \\n\"Lung Opacity\", \"Nodule/Mass\", \"Other lesion\", \"Pleural \\neffusion\", \"Pleural thickening\", \"Pneumothorax\", \\n\"Pulmonary fibrosis\" \\n \\n2.3.2 Data Processing \\nThis situation can be seen as a multi -label classification \\nconsisting of 15 classes. The first step is to create a target \\nvector of all the classes for each instance in the data set. \\nThis is done in a manner similar to one hot encoding \\nwhere we have an array of length 15 consisting of 1s \\nindicating the presence of a disease and 0s indicating the \\nabsence. \\n \\n \\nFig 35: Sample images of classes \\n \\nThis is a multi-label classification task where we have a \\nlimited amount of data due to class imbalance. The raw \\nimage size is too big to fit the batch in memory and hence \\nneed to be resized. A data class is created mapping each \\nimage with its respective 14 masks. Each image is resized \\nto 224x224, gray scaled and converted to tensors. It is \\nthen split in an 80 -20% ratio for training and testing. Due \\nto the lack of data, I opted to forgo the validation set. This \\ndata is then loaded using PyTorch Data Loaders and is \\ndisplayed below. \\n \\n \\n2.3.3 Model \\nI first download the Swin transformer model using \\nPyTorch from the main GitHub repository. This gives the \\nunderlying structure to the model we will use. In addition, \\nI add additional layers to the output of this mode to \\nmodify it for this problem. This includes additional Linear \\nlayers and a Dropout to reduce overfitting. \\n \\n \\nFig 36: Loading Swin Transformer \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 8, 'page_label': '9'}, page_content=' \\n \\n \\nFig 37: Swin Transformer Architecture \\n \\n \\nFig 38: Modifying Swin Architecture \\n \\nIn the selection for loss function, I use Label Smoothing \\nCross Entropy to work with multi label classification and \\nthe Adam optimizer. The learning rate is also reduced \\nwhen it plateaus using the ‘Step learning Rate’ function. \\nThe model is run for 7-12 epochs.   \\n \\n \\nFig 10: Model Training \\n \\n2.3.4 Results \\n \\nFig 39: Model prediction on localization \\n \\n \\nFig 40: Model training loss curve \\n \\n \\nFig 41: FROC curve for localization model \\n \\n3. Project 3 (Pulmonary Embolisms) \\n3.1. PE Presence Classification (Classification) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can  go through 3D CT scan slices and \\nidentify the location of pulmonary embolism. \\n  \\n3.1.1 Dataset Exploration \\nThe dataset consists of 12,195 instances of data including \\nimage data and patient data like their ID, chronic issues, \\nleft and right-side pulmonary embolisms (PE) and \\nindeterminate cases.  \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 9, 'page_label': '10'}, page_content=' \\n \\n \\nFig 42: CT Scan image of a central PE \\nThe dataset consists of the following classes: ‘Negative \\nExam’, ‘Indeterminate’, ‘Righ t side PE’, ‘Left side PE’, \\n‘Central PE’, ‘RV/LV>=1’, ‘RV/LV<1’, ‘Chronic PE’, \\n‘Acute and Chronic PE’, ‘QA Motion’, ‘QA Contrast’ , \\n‘Flow Contrast’ and ‘Filling Defect’ \\n \\n3.1.2 Data Processing \\nTo process this data, we need to define a custom python \\nclass to identi fy the patient data and the multi class data.  \\nThe images must be read from their DICOM filetype to \\nindividual slices. Gray scaling and conversion to Tensors \\nfollow before passing to the model.  These are split into \\npatient wise portions to maintain consistency. \\n \\n3.1.3 Model \\nFor this task, I found most appropriate to use a Resnet 50 \\nbased model pretrained on the ImageNet weights. I use \\nthe Adam Optimizer with lr=0.001 and a learning rate \\nscheduler that reduces it on plateauing. The Binary Cross \\nentropy function is used for loss and run for 70 epochs. \\n \\n \\n \\nFig 43: Resnet 50 architecture used \\n \\n3.1.4 Results \\nThe model achieves and accuracy of 92% and a loss of \\n0.26. The AUC score is 0.93 indicating good \\nperformance. I plot the accuracy over epochs as a check. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 44: Training and validation accuracy over epochs \\n \\nI plot some sample predictions of the model to check its \\nefficacy. \\n \\n \\n \\nFig 45: Model prediction of both classes \\n \\n \\n3.2. PE Segmentation (Segmentation) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can go through 3D CT scan slices and \\nsegment the location of pulmonary embolism. \\n  \\n3.2.1 Dataset Exploration \\nThe FUMPE dataset (Ferdowsi University of Mashhad’s \\nPE) consists of over 8,000 slices of CT scans along with \\nthe respective masks. CT scans are stored as DICOM files \\nwhich need to be processed using the ‘pydicom’ package. \\nThe masks are stored as .mat files. \\n \\n3.2.2 Data Processing \\nTo process this data, we need to define a custom python \\nclass to identify the patient data and the multi class data. \\nThe images must be read from their DICOM filetype to \\nindividual slices. Gray scaling and conversion to Tensors \\nfollow before passing to the model. These are split into \\npatient wise portions to maintain consistency. \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 10, 'page_label': '11'}, page_content=' \\n \\n \\nFig 46: FUMPE dataset unpacking \\n \\n \\n3.2.3 Model \\nFor this task, I found most effective to use a UNet \\narchitecture with a Resnet 50 backbone. It is pretrained on \\nthe ImageNet weights. I use the Adam Optimizer with \\nlr=0.001 and a learning rate scheduler that reduces it on \\nplateauing. The Binary Cross entropy function is used for \\nloss and run for 15 epochs. \\n \\n3.2.4 Results \\nAfter training the model, I used the Dice Score as a \\nmetric to gauge its performance. The model achieved a \\nDice score of 0.77 and a loss of 0.352. While the model \\nperforms well,  there are still instances where the \\nsegmentation masks are inaccurate. I plot a few sample \\npredictions of the model below: \\n \\n \\n \\nFig 47.1: PE segmentation mask prediction \\n \\n \\nFig 47.2: PE segmentation mask prediction \\n \\n3.3. PE Localization (Localization) \\nThe goal of this exercise is to create a State -of-the-art \\n(SoTA) model that can go through 3D CT scan slices and \\nidentify the location of pulmonary embolism.  \\n \\n3.3.1 Dataset  \\nThe CAD PE dataset consists of 41,256 instances of CT \\nscan data including ground truth box coordinates for \\nlocalization.  \\n \\n \\nFig 48: CAD PE dataset  \\n \\n3.3.2 Data Processing \\nTo process this data, we need to define a custom \\npython class to identify the patient data and the multi \\nclass data. The images must be read from their DICOM \\nfiletype to individ ual slices. Gray scaling and conversion \\nto Tensors follow before passing to the model. These are \\nsplit into patient wise portions to maintain consistency. I \\nuse 2000 slices from a single patient for training and 1000 \\nslices from different patients for testing. \\n \\n3.3.3 Model \\nFor this task, I used the Faster RCNN model for \\ngenerating coordinates for the bounding boxes. It uses the \\nResNet50 backbone from scratch. Further experiments \\ncan be done using the pretrained weights as well. As for \\nthe parameters, it uses a St ochastic Gradient Descent \\noptimizer with a learning rate of 0.0001 and a momentum \\nof 0.9. The model is run for 30 epochs. \\n \\n3.3.4 Results \\nAfter training the model, I use the IoU score as metric \\nto gauge its performance.  \\n \\n \\nFig 49: Training loss of PE model over epochs \\n \\n'), Document(metadata={'source': 'documents\\\\Final Report for Project 1,2,3 (Srinivas Natarajan).pdf', 'page': 11, 'page_label': '12'}, page_content=' \\n \\nThe standard model faces issues of sensitivity where \\nmultiple bounding boxes are drawn around regions of \\ninterest (ROI). To solve this, I apply non -maximum \\nsuppression to get more realistic results. \\n \\n \\nFig 50: Non-maximum suppression of predictions \\n \\nEven with these measures, the model achieves an IoU \\nscore of just 0.37. To figure out the performance issues, I \\nplot the false positive rates against the sensitivity. \\n \\n \\nFig 51: False positive rates vs Sensitivity \\n \\nIt is easier to visualize this by plotting a few examples of \\nthe model’s predictions to get a better understanding. \\n \\n \\nFig 52: Ground truths vs model predictions \\n \\n \\nFig 53: Ground truths vs model predictions \\n \\n \\n'), Document(metadata={'source': 'documents\\\\SML_Portfolio_Report.pdf', 'page': 0, 'page_label': '1'}, page_content=' \\nDeep Learning Architectures for Medical \\nSegmentation Tasks \\nSrinivas Natarajan  \\nsnatar28@asu.edu\\nAbstract— The importance of early detection localization of \\npolyps (precursors to colon cancer) cannot be understated. The \\nerror-prone nature of the manual screening process for such \\nabnormalities necessitates an automated system that can help \\npinpoint the location of anomalies with ease. In this project, we \\naim to exploit deep learning models that specifically employ \\nConvolutional Neural Networks (CNNs) and their variants for \\ntheir state-of-the-art performance in image identification tasks. \\nThe objective is to build a model that can be gen eralized to \\ndetect similar anomalies in such medical procedures and tag \\nthem reliably  \\nKeywords— Encoders, CNN, Segmentation , Pyramid \\nNetworks \\nI. INTRODUCTION \\nThe goal of my project is to aid doctors in the identification \\nof tumorous growths in the gastro -intestinal (GI) tract called \\npolyps as they are precursors to colorectal cancer. This is an \\nimportant problem in the medical field as colorectal cancer has \\nthe third highest mortality rate among cancers making it vital \\nto identify early. Compounding this problem is the difficulty \\nin identifying these growths as they are miniscule in size, \\nblending in with the rest of the GI tract.  It would be effective \\nto have a computer-aided system to identify polyps and lesions \\nduring a medical  procedure, especially when used in \\nconjunction with a medical expert. \\nThe general approach taken in medical segmentation \\nproblems is to extract key information from the original image \\nby using Convolutional Neural Networks. This is due to their \\nefficiency in processing larger images by  extracting smaller \\nfeatures from them. Studies have found architectures such as \\nUNet and ResUNet have shown great promise in tackling this \\nproblem but st ill require some tuning  due to the contextual \\nnature of the problem.  However, CNN -based models in \\ngeneral show limitations for explicit long-range relations and \\nthey might exhibit unstable performance, unlike transformers. \\nTherefore, we also use systems that employ encoder networks \\nto extract information and decoder networks to interpret them \\ninto the form we need . The most successful approach of this \\nform uses a series of contraction and expansion layers to \\ndownsize the initial image  while preserving information and \\nscaling it up. \\nII. SOLUTION EXPLAINED \\nI first identified two major datasets collected by universities in \\nconjunction with hospitals. They are: The Clinic CVC dataset \\n[1] from the Endoscopy Vision challenge and the Kvasir-SEG \\ndataset [2] collected by the Vestre Viken Health Trust \\n(Norway), each consists of a sequence of still images taken \\nfrom different endoscopic procedures. As this data is varied in \\nterms of clarity, size and rotation, I first made a preprocessing \\npipeline to download these images, scale them to a 288x384 \\nresolution to have an even padding with kernels of size 32 by \\neither downscaling or padding them and normalized the pixel \\nvalues to a 0 -1 range to make computation easier. This \\nprocessed data is then split into an 80-10-10 split for training, \\nvalidation and testing respectively. Initial runs suffered from \\nunderfitting due to the lack in training data so I add ed an \\naugmentation step into the pipeline. This involved flipping the \\ntraining images horizontally and vertically, adding random \\nrotations and crops and randomly altering the brightness. This \\nalso gave an added advantage of making the model more \\nadaptable to real world conditions as there are many variables \\nin this procedure.  \\n With the pre-processing done, I then set up five different \\narchitectures with varying approaches to validate their \\nfeasibility. The first model was the UNet  [3], a baseline \\nstandard for most medical image segmentation tasks. This \\nsymmetric architecture consists of two main components - \\ncontraction and expansion. The contraction section is a classic \\nCNN architecture consisting of two 3x3 convolution layers, a \\nReLU and 2x2 max pooling unit. At each stride of the max \\npooling operation, we double the number of feature channels \\nso in the expansion section, we have 2x2 up-convolution layer \\nthat halves the number of feature channels. Then, we have \\nconcatenation, two 3x3 convolution layers. Each of  these \\nlayers are followed by a ReLU. Finally, we have 1x1 \\nconvolution layer to map each feature vector to a suitable \\nnumber of classes. \\n \\nFigure: UNet Architecture \\nThe next model implemented is the UNet++  [4] which adds \\nresidual gateways and skip pathways between the encoder and \\ndecoder to the UNet architecture. It employs deep supervision \\nto perform more accurate segmentation. The re-designed skip \\npathways aim at reducing the semantic gap between the \\nfeature maps of the encoder and decoder sub -networks, and \\nhave proved effective in recovering fine-grained details of the \\ntarget objects. The UNet model serves as a baseline as it the \\nmost common basepoint taken when tackling medical \\nsegmentation tasks. The UNet++ is taken as our first \\nmodification as it is a smal l upgrade over the base UNet but \\nproduces a much-improved results. \\n'), Document(metadata={'source': 'documents\\\\SML_Portfolio_Report.pdf', 'page': 1, 'page_label': '2'}, page_content='2 \\n \\n \\nFigure: UNet++ Architecture \\nThe next two models follow the methodology of pyramidal \\narchitectures of the encoder and decoder networks for \\nsegmentation. These are the Pyramid Attention Net work \\n(PAN) [5] and the Feature Pyramid Network (FPN)  [6]. The \\nFeature Pyramid Network produces proportionally scaled \\nfeature maps at each level. This map produced during the \\ncontraction phase is saved and used in the reconstruction of \\nthe more information dense im age created during the \\nexpansion phase. It focuses on upscaling semantically \\nstronger feature maps from higher levels to give the illusion \\nof great feature resolution.  It consists of two pathways: the \\nbottom-up pathway usually made up of CNNs to \\nextract features where the spatial resolution decreases but \\nsemantic values increases and the top -down pathways where \\nthe object is down sampled. There are lateral connections \\nbetween the reconstruction layers and the feature maps to \\nbetter predict locations. \\n \\nFigure: Feature Pyramid Network Architecture \\nThe Pyramid Attention Network uses two important modules: \\nthe Global Attention Upscale (GAU) and the Feature Pyramid \\nAttention (FPN). The FPN modules is a U-shaped architecture \\nthat extracts information using 3x3, 5x5, 7x7 filters. The \\npyramid structure uses information extracted every step \\nwithout the additional  burden of using larger filters as the \\nresolution of the feature map is already small. The GAU unit \\nperforms global average pooling to provide global context to \\nthe process. This is then passes through a 1x1 convolution \\nfilter, batch normalized and passed through a ReLU activation \\nfunction to add non-linearity. \\n \\nFigure: Pyramid Attention Network Architecture \\nWith the pyramid extraction methods out of the way, the last \\nmodel left is one that represents the current state of the art \\nwork. For this we chose th e DeepLabv3 architecture  [7] \\ndeveloped by Google . This novel approach solves the \\ninformation loss when in two ways. When downscaling the \\nimage, a combination of atrous convolutional network and \\nspatial pyramid pooling modules helps reduce the info loss. \\nThe ad vantage of this method is that through the dilation \\nparameter, the network can extract the information that could \\nbe potentially obtained with a larger convolution kernel while \\navoiding the cost that comes with actually using a larger \\nkernel. While upscali ng, the use of a technique called Point  \\nRend enhancement helps reduce the loss of info in the process.  \\n \\nFigure: DeepLabv3 Architecture \\nIII. RESULTS \\nThe five model were used to predict polyp boundaries on our \\ntest dataset. We used two metrics to compare these models: \\nIoU and Dice scores. The IoU score which stands for \\nIntersection over Union, signifying the ratio of the intersecting \\nregion over the ove rlapping region.  An IoU score of 1 \\nindicates a perfect boundary prediction. The IoU score can be \\ngiven as follows: \\n𝐼𝑜𝑈 =  𝑇𝑃\\n𝑇𝑃 +  𝐹𝑃 +  𝐹𝑁 \\nEquation: Intersection over Union Score \\nIt calculates the sum of correctly predicted boundary pixels  \\nover the sum of total boundary pixels of both prediction and \\nground truth. This is a good metric to use as it considers both \\nlocal and global information loss. The Dice loss can be given \\nas follows: \\n𝐷 =  2 ∑ 𝑝𝑖 ∗ 𝑔𝑖\\n𝑁\\n𝑖\\n∑ 𝑝𝑖\\n2𝑁\\n𝑖  +  ∑ 𝑔𝑖\\n2𝑁\\n𝑖\\n \\nEquation: Dice Loss \\n We found that the best models among the ones tested were \\nthe UNet++ and the DeepLabv3.  \\n \\nFigure: UNet++ IoU score \\n'), Document(metadata={'source': 'documents\\\\SML_Portfolio_Report.pdf', 'page': 2, 'page_label': '3'}, page_content='3 \\n \\n \\nFigure: DeepLabv3 IoU score \\nThough the results were close when we look at the IoU scores, \\ncoming withing a margin of error . We can see that the \\nDeepLabv3 model with the help of a better upscaling \\nalgorithm is able to make better use of the data available by \\nbetter maintaining the loc ation details in the image. This is \\nclear in the time it takes for the IoU score to plateau, especially \\ncompared to the UNet++ score in the first few epochs.  Let’s \\ncompare the dice losses for a more definitive. \\n \\nFigure: UNet++ Dice Loss \\n \\nFigure: DeepLabv3 Dice Loss \\nWe use the Dice score as a tie breaker as it penalizes mistakes \\nmore heavily. This is critical when developing any application \\nfor medical purposes as the margin of error is minimal. With \\nthis we come to the conclusion that the best model to us e is \\nthe DeepLabv3. \\n \\nFigure: Performance comparisons of the model  \\nIn conclusion, we explained the shortcoming of each \\narchitecture as well as specific use cases where they shine. We \\nfound that the DeepLabv3 architecture was the best in terms \\nof our metrics which emphasized minimizing the error rate. \\nWe went forward with this metric as the margins for error in \\nthe medical field are miniscule and errors should be heavily \\npenalized. \\nIV. CONTRIBUTION AND SKILLS \\nMy role in this project included:  \\n1. Creating the  data collection, preprocessing and \\naugmentation pipeline for two datasets of different \\nformats.  \\n2. Implementing the DeepLabv3 architecture from the \\nresearch paper recently published by Google using \\nthe PyTorch library and its functions. \\n3. Designing the graphi ng and results collection \\nfunction for the various models to come to a cohesive \\nresult. \\nThe other members involved in this group project are Sai \\nPranav Tavva (1225344341),  Tanushi Ahuja (1225475680), \\nPranavi Addagatla (1225696667)  and Shivani Yerram \\n(1225766373) \\nOver the course of this project, I learnt more about processing \\nimages for training deep learning models. This was vital as the \\namount of processing power needed to effectively tackle these \\nproblems is immense and the data needs to be modified to \\nmake it feasible in finite time and ordinary hardware. I also \\nlearned about the various encoder -decoder architectures for \\nimage feature extraction and their strengths and weaknesses \\nfor various applications. Through experimentation with the \\nPyTorch package w hen creating the various model \\narchitectures, I also learned about additional packages which \\nwork with PyTorch like the “Albumnations” and the \\n“Segmentation” packages which offer important \\nfunctionalities which made creating data pipelines much \\neasier.  \\nV. REFERENCES \\n[1] Bernal, J., Sánchez, F. J., Fernández-Esparrach, G., Gil, \\nD., Rodríguez, C., & Vilariño, F. (2015). WM -DOVA \\nmaps for accurate polyp highlighting in colonoscopy: \\nValidation vs. saliency maps from physicians. \\nComputerized Medical Imaging and Graphics, 43, 99 -\\n111. \\n[2] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. \\nde Lange, D. Johansen, and H. D. Johansen, “Kvasir -\\nSEG: A segmented polyp dataset,” MultiMedia \\nModeling, pp. 451–462, 2019. \\n[3] O. Ronneberger, P. Fischer, and T. Brox, “U -Net: \\nConvolutional Network s for Biomedical Image \\nSegmentation,” arXiv.org, 18 -May-2015. [Online]. \\nAvailable: https://doi.org/10.48550/arXiv.1505.04597. \\n[4] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh and J. \\nLiang, \"UNet++: Redesigning Skip Connections to \\nExploit Multiscale Features in Image Segmentation,\" in \\nIEEE Transactions on Medical Imaging, vol. 39, no. 6, \\npp. 1856 -1867, June 2020, doi: \\n10.1109/TMI.2019.2959609. \\n[5] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid  \\nAttention Network for semantic segmentation,” \\narXiv.org,(2018).https://doi.org/10.48550/arXiv.1805.\\n10180. \\n[6] Taresh Sarvesh Sharan, Sumit Tripathi, Shiru Sharma \\n& Neeraj Sharma (2022) Encoder Modified U -Net and \\nFeature Pyramid Network for Multi-class Segmentation \\nof Cardiac Magnetic Resonance Images, IETE \\n'), Document(metadata={'source': 'documents\\\\SML_Portfolio_Report.pdf', 'page': 3, 'page_label': '4'}, page_content='4 \\n \\nTechnical Review, 39:5, 1092 -1104, DOI: \\n10.1080/02564602.2021.1955760 \\n[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. \\nAdam, “Encoder -decoder with atrous separable \\nconvolution for Semantic Image segmentation,” \\nComputer Vision – ECCV 2018, pp. 833–851, 2018. '), Document(metadata={'source': 'documents\\\\Srinivas_Natarajan_Resume.pdf', 'page': 0, 'page_label': '1'}, page_content='Srinivas Natarajan Data Engineer\\nTempe, Arizona 85282\\n+1 602 814 9054 nsrinivas06@gmail.com linkedin.com/in/srinivas-natarajan github.com/Srinivas-Natarajan\\nEducation\\nArizona State University August 2022 – May 2024\\nMaster of Science in Computer Science GPA:3.96/4.0\\nRelevant Coursework: Cloud Computing, Statistical Machine Learning, Data Mining, Artificial Intelligence\\nVellore Institute of Technology July 2018 – June 2022\\nBachelor of Technology in Computer Science GPA: 9.38/10.0\\nRelevant Coursework: Computer Architecture, Database Systems, Operating Systems, Natural Language Processing\\nTechnical Skills\\nLanguages/Tools: Python, C++, JavaScript, Java, SQL, PostgreSQL MongoDB, NoSQL, HTML/CSS, Bootstrap, Node.js,\\nMicrosoft Power BI, Tableau, MATLAB, Git, GitHub, AWS, Oracle Cloud (OCI), Docker, Linux, LangChain, FastAPI,\\nTensorFlow, PyTorch, Matplotlib, OpenCV, Excel, REST API, JIRA, Django, Flask, Spark, Confluence, Apache Kafka,\\nApache Airflow, AWS Glue, CloudWatch\\nSkills: Machine Learning, Deep Learning, NLP, LLMs, Data Engineering, Data Analysis, Data Visualization, Data\\nGovernance, Data Warehouse, Data Modeling, Data Pipelines, DevOps, Continuous Integration, Agile Development, Shell\\nScripting\\nCertifications: Oracle Generative AI Certified Professional, AWS Cloud Practitioner\\nExperience\\nLocal Grown Salads August 2024 – Now\\nBackend Engineer Tempe, AZ\\n• Engineered a time-critical feature to meet FDA regulatory standards, ensuring compliance for over 200,000 data entries\\nwithin a 3-month deadline.\\n• Collaborated to develop a data streaming and messaging system with Apache Kafka, enabling efficient communication\\nacross 5 cross-functional teams and monitoring using Airflow.\\n• Spearheaded the containerization of 3 key source code components with Docker, reducing deployment time by 20% and\\nfacilitating seamless migration to AWS.\\nVelozity Global Solutions February 2022 – May 2022\\nMachine Learning Engineering Intern Chennai, India\\n• Designed a Python application to detect arrhythmia using live ECG signals, achieving a 97% accuracy rate.\\n• Programmed a robust classification system capable of identifying 15 types of arrhythmia, utilizing transformer-based\\nmodels integrated with a pipeline for data engineering and peak detection algorithms.\\n• Prototyped a health device and helped design an associated dashboard that improved data monitoring capabilities,\\nresulting in a 5% reduction in the response time.\\nAmigo July 2021 – September 2021\\nSoftware Developer Intern Vellore, India\\n• Developed deep learning models for audio augmentation, recognition, and classification of 25 classes.\\n• Coordinated in building a CI/CD and data pipeline for ETL operations to integrate audio pre-processing and\\naugmentation into the product, streamlining future tuning and improvements, and achieving 82% accuracy.\\n• Led a comparative study on the performance of traditional pre-trained CNNs versus transformer-based solutions.\\nSmartERP Solutions May 2021 – July 2021\\nTrainee Consultant Bangalore, India\\n• Automated a dashboard for cost analysis and KPI visualization using Power BI, to aid reporting, saving 3 hours weekly.\\n• Implemented RBAC and OLS in the organization’s cloud infrastructure, reducing unauthorized access incidents and\\nenhancing overall security compliance.\\n• Implemented DAX measures to streamline financial reporting processes, reducing data retrieval time and increasing team\\nproductivity by over 10 hours monthly.\\nProjects\\nOpen Source Facial Recognition | Python, AWS, Ceph, OpenFaaS, Docker November 2023\\n• Deployed over 15 instances of custom VM infrastructures dedicated to hosting educational software powered by Python\\n& Ceph clusters as an alternative to AWS EC2, Lambda and S3.\\n• Architected a multi-cluster database on AWS, integrating Ceph for scalable storage, and designed a high-accuracy (99%)\\npredictive model.\\nHeart Arrhythmia Detection System | Python, PyTorch, TensorFlow, JavaScript, Docker March 2022\\n• Spearheaded the design and implementation of a deep learning solution to classify electrocardiogram (ECG) signals into\\n15 different arrhythmia labels.\\n• Constructed a specialized system that was seamlessly compatible with smart devices and Arduino platforms, enhancing\\nusability across healthcare applications.')]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWyeEQ6CB3Kv"
      },
      "source": [
        "# Text Splitting and Chunk Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iyYkhgNkGed-"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vCvrShBIKPi",
        "outputId": "7d37f2d9-2886-4f3e-8421-72649b63518f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'documents\\\\Srinivas_Natarajan_Resume.pdf', 'page': 0, 'page_label': '1'}, page_content='Srinivas Natarajan Data Engineer\\nTempe, Arizona 85282\\n+1 602 814 9054 nsrinivas06@gmail.com linkedin.com/in/srinivas-natarajan github.com/Srinivas-Natarajan\\nEducation\\nArizona State University August 2022 – May 2024\\nMaster of Science in Computer Science GPA:3.96/4.0\\nRelevant Coursework: Cloud Computing, Statistical Machine Learning, Data Mining, Artificial Intelligence\\nVellore Institute of Technology July 2018 – June 2022\\nBachelor of Technology in Computer Science GPA: 9.38/10.0\\nRelevant Coursework: Computer Architecture, Database Systems, Operating Systems, Natural Language Processing\\nTechnical Skills\\nLanguages/Tools: Python, C++, JavaScript, Java, SQL, PostgreSQL MongoDB, NoSQL, HTML/CSS, Bootstrap, Node.js,\\nMicrosoft Power BI, Tableau, MATLAB, Git, GitHub, AWS, Oracle Cloud (OCI), Docker, Linux, LangChain, FastAPI,\\nTensorFlow, PyTorch, Matplotlib, OpenCV, Excel, REST API, JIRA, Django, Flask, Spark, Confluence, Apache Kafka,\\nApache Airflow, AWS Glue, CloudWatch\\nSkills: Machine Learning, Deep Learning, NLP, LLMs, Data Engineering, Data Analysis, Data Visualization, Data\\nGovernance, Data Warehouse, Data Modeling, Data Pipelines, DevOps, Continuous Integration, Agile Development, Shell\\nScripting\\nCertifications: Oracle Generative AI Certified Professional, AWS Cloud Practitioner\\nExperience\\nLocal Grown Salads August 2024 – Now\\nBackend Engineer Tempe, AZ\\n• Engineered a time-critical feature to meet FDA regulatory standards, ensuring compliance for over 200,000 data entries\\nwithin a 3-month deadline.\\n• Collaborated to develop a data streaming and messaging system with Apache Kafka, enabling efficient communication\\nacross 5 cross-functional teams and monitoring using Airflow.\\n• Spearheaded the containerization of 3 key source code components with Docker, reducing deployment time by 20% and\\nfacilitating seamless migration to AWS.\\nVelozity Global Solutions February 2022 – May 2022\\nMachine Learning Engineering Intern Chennai, India\\n• Designed a Python application to detect arrhythmia using live ECG signals, achieving a 97% accuracy rate.\\n• Programmed a robust classification system capable of identifying 15 types of arrhythmia, utilizing transformer-based\\nmodels integrated with a pipeline for data engineering and peak detection algorithms.\\n• Prototyped a health device and helped design an associated dashboard that improved data monitoring capabilities,\\nresulting in a 5% reduction in the response time.\\nAmigo July 2021 – September 2021\\nSoftware Developer Intern Vellore, India\\n• Developed deep learning models for audio augmentation, recognition, and classification of 25 classes.\\n• Coordinated in building a CI/CD and data pipeline for ETL operations to integrate audio pre-processing and\\naugmentation into the product, streamlining future tuning and improvements, and achieving 82% accuracy.\\n• Led a comparative study on the performance of traditional pre-trained CNNs versus transformer-based solutions.\\nSmartERP Solutions May 2021 – July 2021\\nTrainee Consultant Bangalore, India\\n• Automated a dashboard for cost analysis and KPI visualization using Power BI, to aid reporting, saving 3 hours weekly.\\n• Implemented RBAC and OLS in the organization’s cloud infrastructure, reducing unauthorized access incidents and\\nenhancing overall security compliance.\\n• Implemented DAX measures to streamline financial reporting processes, reducing data retrieval time and increasing team\\nproductivity by over 10 hours monthly.\\nProjects\\nOpen Source Facial Recognition | Python, AWS, Ceph, OpenFaaS, Docker November 2023\\n• Deployed over 15 instances of custom VM infrastructures dedicated to hosting educational software powered by Python\\n& Ceph clusters as an alternative to AWS EC2, Lambda and S3.\\n• Architected a multi-cluster database on AWS, integrating Ceph for scalable storage, and designed a high-accuracy (99%)\\npredictive model.\\nHeart Arrhythmia Detection System | Python, PyTorch, TensorFlow, JavaScript, Docker March 2022\\n• Spearheaded the design and implementation of a deep learning solution to classify electrocardiogram (ECG) signals into\\n15 different arrhythmia labels.\\n• Constructed a specialized system that was seamlessly compatible with smart devices and Arduino platforms, enhancing\\nusability across healthcare applications.')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[len(text_chunks)-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVHiWp3vCLof"
      },
      "source": [
        "# Generate Vectors: Use a local embedding model to create embeddings for document content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "fbffa3afffe841bb96a650f2725a1d95",
            "741ecc3b9cd14bfb84d1a38437bfaa6f",
            "ac21399e28ad4d9e9b5efea14995a5fd",
            "e0d9bdffab034566a0912667c994b0df",
            "c2406b587f5342ae9992ad077e6ddbe6",
            "bf6fe127e29442e6a7b4fce25bf247b2",
            "f9857b4917ce47988d3b5c3a2cada072",
            "c90d06e5c57941bf8f77ebbebbbcbc1f",
            "4f017f5f43a246a3bc532c2a52565daa",
            "df7edbac29cf4716b012f33f8ba14a85",
            "337bf17dd41547d794aa35196bc26a48",
            "8c02a0ec9ad94e169d6d09f859b7425b",
            "876a6a034a7348ae8c791fd728df4a31",
            "340eef53102b4e5babfa903300549e9f",
            "ea691b76d7394327ac7b18e916b811b6",
            "41ec9baf2afa4639953b76f46e9f16f5",
            "8728d8fef01f422489d209593bd16a61",
            "4b6ef4d983244d2abb2c9c809b0fdbd2",
            "02bdd3e7db5b453ba06b6b7c43d71ee2",
            "9c2f47a11da941eba186379dcfe89ae3",
            "d10366be3405407bb593ec89157387ea",
            "ceab4691bb92485fb54e6ea10ff6a508",
            "8c9535abe7c140bbb2691f5f1cf5475d",
            "3b30794bc50c4af8a992438681157606",
            "59eaa8a4c08a4a06881f659e98c7e9ee",
            "9ef6007da43a433c9b914f096e871e3e",
            "7d831b79d171468ebbaea831f6d90e2e",
            "5a104549fba6437f9b8e349411865e62",
            "6956ce9de38442468b9f2fbeb3d93454",
            "e3d434f4ab6b4ad9b405bd3bb5551aff",
            "e6c6900712734f08a6b706628b2e91fa",
            "2c3c04dfa1834e8e9a1c7e25e9de0c8c",
            "c5750ecd88af429daa56049369ce583f",
            "49bea2a9761547a496f07869905b5b68",
            "884532546f864d82bcd8214c862001b8",
            "94d006af4d8c4b59bc7395cbd56d3711",
            "0c718dd14fbb439e969a33ea8a95debd",
            "2de5357038fa491fb3c8b69b0585e5f6",
            "6e142947baea4eb5b9ab9df1e77c1da1",
            "72f9201df51c411da6a53ade73b5ae30",
            "c30fe920324a40dbaa6387a7a3f3157c",
            "5fa261e9273c499ebd6944849dab849a",
            "7943d5371eac425b910def5df2e33b13",
            "a9a32eb87eb041c98bc8797999a2a237",
            "ede51521ec5d4d4387fd1bb9664e4765",
            "2c20fd03694a42dc9838e030bfe91a1a",
            "088cb6eec8bd43c7878f28ead8ec77cc",
            "db163de0d0404d45820a32a1a4b020aa",
            "5087706f90e3474f821f987a1674e49d",
            "3d9ca4da03ec4d2fa5cae2c62611df2a",
            "8bc1c069b07543edb08a5b7b56e44b70",
            "9f69bbcf1a364fbc9f5a4b87a9707e8d",
            "f4ad77781efc4c32a4c0146c12cd4ef7",
            "f64e9893f4344cda9376e6665b9e9cc8",
            "1a3455264b064db5956bf1bcdce49f88",
            "3debe92dffc5491f8ad4d87d2cd84618",
            "54fbcebcfae84d3987b1a9221306b289",
            "f8feff483deb41f6ae560c1672dfc140",
            "9acb21553880459daaf2d0d4cf5ebb8a",
            "6b77403185df4c778198983183d66b9d",
            "bb63070499754237a8ec9cfaabf2c8af",
            "ec63ed22e44d4a46882532e3e450868a",
            "0b91fef71310492d83331f6a4b55c24a",
            "19a211dff68544409f155d40014d39e8",
            "28bf3746ba70469fba60bdd39104bebb",
            "d572e981411c41859bd309f1969cc10b",
            "475921a2e8c44d7dadfb27aeaba4e49c",
            "970df4d75ac4412293cf3e8675cbe108",
            "a85c8f3c6367410fbccfc422c5c94840",
            "ba169ce061744348be80bfbcb51ff43d",
            "eda9ec3506504f11810f8d3aea112821",
            "9d2fb484ad5b4f8899d28cc498876a69",
            "1a5a642761744fe393fd7ff8dec00a7d",
            "0d080a1849424cfe9b9eecf4cfd4177b",
            "86c2221e3bf94bd4a89591a4ccb36bf0",
            "52f137ac74e5415b8f6948a979d640d1",
            "415ae90ba08b4734ab80447e1c340ee7",
            "66bed060881a45b7b63ec086b965e7c8",
            "18cda3b7ad4847faae5c53601bb647c1",
            "b930cbfca7f74078b93d5602773d296b",
            "42561fe8df2e406ebb715f3e3990e811",
            "dcfbb9f56c224e7889b4154dca553bdb",
            "8e0792165eb940d5a60153ba57a112f2",
            "765cb7f89006430583975a2fbd2bf8a8",
            "22629bfb6cec47ea8c23a506e7b4abed",
            "2f637db2c6784169b3a9089e422d9c68",
            "046a68148a184de38958fbc118925190",
            "8492dba614e6424193605dba29a78f26",
            "abf71bfa63f84f31b9c91e3c81cfa095",
            "9545977952d243c7a78adeab8e9c01d6",
            "fa8e9e83ca4e4bcc9b2ea0556b8b9876",
            "3744a5bc5d384c7980dc9062682d9cc4",
            "8114d70d240e4badae3b1a868917ce96",
            "05d86ca46c3741ebabcb599313025b66",
            "88fd9075b189479e91d2a2a560838d3a",
            "4c2829de85fe43689143a27e2332c814",
            "050051cf9cfb442b9a50e514f9e596a0",
            "1c8292e21554427f8d7104fd46d37003",
            "a0b1ee232c7f4eebb0b8ee91017677b1",
            "2cb3b9a8d0264b329cae5609f1a12f3e",
            "5da33a491c09411babe8e0bf4f2e8f0d",
            "084d24e468f84789900ff13de454398f",
            "521fd37d54934505beb25de47250d111",
            "c90448ad471343c78d39f96059837750",
            "da5f5463470544a38c61402ad6d45761",
            "6fbc302096944372882b2e5bea97d69c",
            "f828d752b80a4998b9ef583ffd7c5d73",
            "c01fc88f61ef4b63be7d56386df0852a",
            "4bd357a9d0aa47d6b7ea22c3fa1d6fe6",
            "9f5cbf8067a14c3b971683ba2d34caa6",
            "ea26c73dd146447293f9bc842d78434b",
            "83fad3fdef714fd684ebedd91f283e51",
            "bb644df6e0aa44de864a9b27f5a7d647",
            "c671b5e4126c48428ff67ba13ee6f609",
            "a746fb03a0cc4beeb6963e529790bc9e",
            "3d64f76e0d95452d9440d79ff31fe469",
            "885b8ad6c8ae448da2116b5ea9e6d52e",
            "df0dad828e7c4d39904ee4441d6b4a70",
            "4b04687b65d74da8bd11810627439bbc",
            "bee80aed71fb40aa8ed13a832cf35d01",
            "050d6aeca905427e960963c2b0be16e0"
          ]
        },
        "id": "Fj3aI-JaILxU",
        "outputId": "39078525-7199-486a-80ed-c68b812a91a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Software_Installs\\Anaconda\\envs\\llms\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "d:\\Software_Installs\\Anaconda\\envs\\llms\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nsrin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAR7C51rCcKs"
      },
      "source": [
        "# Store in Vector Store: Utilize local persisting methods in the chosen vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNv6CmpmIssl"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5uZq1TyF2_7"
      },
      "source": [
        "# Integrate LLM: Run a local instance of a Large Language Model for contextual insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksvzODOqJVfR",
        "outputId": "04c3f3bf-e908-499a-cb06-78d083127d1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/drive/MyDrive/Colab Notebooks/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    18.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "#import model\n",
        "\n",
        "llm = LlamaCpp(\n",
        "\n",
        "               streaming=True,\n",
        "               model_path='/content/drive/MyDrive/Colab Notebooks/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "               temperature = 0.75,\n",
        "               top_p=1,\n",
        "               verbose=True,\n",
        "               n_ctx=4096\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa0ygx77GEZm"
      },
      "source": [
        "# Configure Query Engine: Set up retrieval tasks based on document embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsapYFKBNenF"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\":2}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHbyVeWaOI1q"
      },
      "outputs": [],
      "source": [
        "query = \"What are the differences in the business of Tesla and Uber?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "vTJtDjlCOUHK",
        "outputId": "3b9b075c-cfb6-411c-d9b2-52745b240b59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "\n",
            "llama_print_timings:        load time =    3306.32 ms\n",
            "llama_print_timings:      sample time =      72.77 ms /   131 runs   (    0.56 ms per token,  1800.22 tokens per second)\n",
            "llama_print_timings: prompt eval time = 1154473.83 ms /  2475 tokens (  466.45 ms per token,     2.14 tokens per second)\n",
            "llama_print_timings:        eval time =   93895.68 ms /   130 runs   (  722.27 ms per token,     1.38 tokens per second)\n",
            "llama_print_timings:       total time = 1248775.63 ms /  2605 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Tesla and Uber are two very different companies with different business models. Tesla is primarily involved in the manufacturing and sale of electric vehicles, energy storage products, and solar panels. They also provide services such as maintenance and repair of vehicles, installation of energy storage products, and energy management for commercial and industrial customers. On the other hand, Uber's primary business is providing a technology platform that connects consumers with independent providers of ride services, meal preparation, grocery, and other delivery services. Uber also provides financial partnership products and advertising opportunities to consumers and merchants. The two companies operate in different industries and have different business models.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj1dl0uXGJot"
      },
      "source": [
        "# Develop Chatbot Interface: Use Streamlit to facilitate user interaction and display comparative insights.\n",
        "\n",
        "App: https://github.com/Vibhuarvind/Content-Engine-RAG-for-PDF/blob/main/app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "auxSM4XcOYXT",
        "outputId": "c904a97e-4972-4d0b-df63-cf061a922620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt : What is the total revenue for Google Search?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3306.32 ms\n",
            "llama_print_timings:      sample time =      16.06 ms /    24 runs   (    0.67 ms per token,  1494.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =  858014.79 ms /  1845 tokens (  465.05 ms per token,     2.15 tokens per second)\n",
            "llama_print_timings:        eval time =   16018.39 ms /    23 runs   (  696.45 ms per token,     1.44 tokens per second)\n",
            "llama_print_timings:       total time =  874102.63 ms /  1868 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The total revenue for Google Search in 2023 is $175,033 million.\n",
            "Input Prompt : exit\n",
            "Exiting...\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt : \")\n",
        "  if(user_input == \"exit\"):\n",
        "    print(\"Exiting...\")\n",
        "    sys.exit()\n",
        "  if(user_input == ''):\n",
        "    continue\n",
        "  result = qa({'query':user_input})\n",
        "  print(f\"Answer:{result['result']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk5QX3NkWoUL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNRw8h1qqO1Ytmc8IL608lZ",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1SPOlv-9enqKhBnI1asrbBS57vMf5JGP-",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02bdd3e7db5b453ba06b6b7c43d71ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "046a68148a184de38958fbc118925190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "050051cf9cfb442b9a50e514f9e596a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "050d6aeca905427e960963c2b0be16e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05d86ca46c3741ebabcb599313025b66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084d24e468f84789900ff13de454398f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f828d752b80a4998b9ef583ffd7c5d73",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c01fc88f61ef4b63be7d56386df0852a",
            "value": 112
          }
        },
        "088cb6eec8bd43c7878f28ead8ec77cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f69bbcf1a364fbc9f5a4b87a9707e8d",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4ad77781efc4c32a4c0146c12cd4ef7",
            "value": 612
          }
        },
        "0b91fef71310492d83331f6a4b55c24a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c718dd14fbb439e969a33ea8a95debd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7943d5371eac425b910def5df2e33b13",
            "placeholder": "​",
            "style": "IPY_MODEL_a9a32eb87eb041c98bc8797999a2a237",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.37kB/s]"
          }
        },
        "0d080a1849424cfe9b9eecf4cfd4177b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18cda3b7ad4847faae5c53601bb647c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e0792165eb940d5a60153ba57a112f2",
            "placeholder": "​",
            "style": "IPY_MODEL_765cb7f89006430583975a2fbd2bf8a8",
            "value": "vocab.txt: 100%"
          }
        },
        "19a211dff68544409f155d40014d39e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a3455264b064db5956bf1bcdce49f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a5a642761744fe393fd7ff8dec00a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8292e21554427f8d7104fd46d37003": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22629bfb6cec47ea8c23a506e7b4abed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bf3746ba70469fba60bdd39104bebb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c20fd03694a42dc9838e030bfe91a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d9ca4da03ec4d2fa5cae2c62611df2a",
            "placeholder": "​",
            "style": "IPY_MODEL_8bc1c069b07543edb08a5b7b56e44b70",
            "value": "config.json: 100%"
          }
        },
        "2c3c04dfa1834e8e9a1c7e25e9de0c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb3b9a8d0264b329cae5609f1a12f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5da33a491c09411babe8e0bf4f2e8f0d",
              "IPY_MODEL_084d24e468f84789900ff13de454398f",
              "IPY_MODEL_521fd37d54934505beb25de47250d111"
            ],
            "layout": "IPY_MODEL_c90448ad471343c78d39f96059837750"
          }
        },
        "2de5357038fa491fb3c8b69b0585e5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f637db2c6784169b3a9089e422d9c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "337bf17dd41547d794aa35196bc26a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "340eef53102b4e5babfa903300549e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bdd3e7db5b453ba06b6b7c43d71ee2",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c2f47a11da941eba186379dcfe89ae3",
            "value": 116
          }
        },
        "3744a5bc5d384c7980dc9062682d9cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8292e21554427f8d7104fd46d37003",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b1ee232c7f4eebb0b8ee91017677b1",
            "value": " 466k/466k [00:00&lt;00:00, 28.8MB/s]"
          }
        },
        "3b30794bc50c4af8a992438681157606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a104549fba6437f9b8e349411865e62",
            "placeholder": "​",
            "style": "IPY_MODEL_6956ce9de38442468b9f2fbeb3d93454",
            "value": "README.md: 100%"
          }
        },
        "3d64f76e0d95452d9440d79ff31fe469": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9ca4da03ec4d2fa5cae2c62611df2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3debe92dffc5491f8ad4d87d2cd84618": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54fbcebcfae84d3987b1a9221306b289",
              "IPY_MODEL_f8feff483deb41f6ae560c1672dfc140",
              "IPY_MODEL_9acb21553880459daaf2d0d4cf5ebb8a"
            ],
            "layout": "IPY_MODEL_6b77403185df4c778198983183d66b9d"
          }
        },
        "415ae90ba08b4734ab80447e1c340ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ec9baf2afa4639953b76f46e9f16f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42561fe8df2e406ebb715f3e3990e811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_046a68148a184de38958fbc118925190",
            "placeholder": "​",
            "style": "IPY_MODEL_8492dba614e6424193605dba29a78f26",
            "value": " 232k/232k [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "475921a2e8c44d7dadfb27aeaba4e49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970df4d75ac4412293cf3e8675cbe108",
              "IPY_MODEL_a85c8f3c6367410fbccfc422c5c94840",
              "IPY_MODEL_ba169ce061744348be80bfbcb51ff43d"
            ],
            "layout": "IPY_MODEL_eda9ec3506504f11810f8d3aea112821"
          }
        },
        "49bea2a9761547a496f07869905b5b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_884532546f864d82bcd8214c862001b8",
              "IPY_MODEL_94d006af4d8c4b59bc7395cbd56d3711",
              "IPY_MODEL_0c718dd14fbb439e969a33ea8a95debd"
            ],
            "layout": "IPY_MODEL_2de5357038fa491fb3c8b69b0585e5f6"
          }
        },
        "4b04687b65d74da8bd11810627439bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b6ef4d983244d2abb2c9c809b0fdbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd357a9d0aa47d6b7ea22c3fa1d6fe6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c2829de85fe43689143a27e2332c814": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f017f5f43a246a3bc532c2a52565daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5087706f90e3474f821f987a1674e49d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521fd37d54934505beb25de47250d111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd357a9d0aa47d6b7ea22c3fa1d6fe6",
            "placeholder": "​",
            "style": "IPY_MODEL_9f5cbf8067a14c3b971683ba2d34caa6",
            "value": " 112/112 [00:00&lt;00:00, 8.31kB/s]"
          }
        },
        "52f137ac74e5415b8f6948a979d640d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fbcebcfae84d3987b1a9221306b289": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb63070499754237a8ec9cfaabf2c8af",
            "placeholder": "​",
            "style": "IPY_MODEL_ec63ed22e44d4a46882532e3e450868a",
            "value": "model.safetensors: 100%"
          }
        },
        "59eaa8a4c08a4a06881f659e98c7e9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d434f4ab6b4ad9b405bd3bb5551aff",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6c6900712734f08a6b706628b2e91fa",
            "value": 10659
          }
        },
        "5a104549fba6437f9b8e349411865e62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da33a491c09411babe8e0bf4f2e8f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5f5463470544a38c61402ad6d45761",
            "placeholder": "​",
            "style": "IPY_MODEL_6fbc302096944372882b2e5bea97d69c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5fa261e9273c499ebd6944849dab849a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66bed060881a45b7b63ec086b965e7c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18cda3b7ad4847faae5c53601bb647c1",
              "IPY_MODEL_b930cbfca7f74078b93d5602773d296b",
              "IPY_MODEL_42561fe8df2e406ebb715f3e3990e811"
            ],
            "layout": "IPY_MODEL_dcfbb9f56c224e7889b4154dca553bdb"
          }
        },
        "6956ce9de38442468b9f2fbeb3d93454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b77403185df4c778198983183d66b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e142947baea4eb5b9ab9df1e77c1da1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fbc302096944372882b2e5bea97d69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72f9201df51c411da6a53ade73b5ae30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "741ecc3b9cd14bfb84d1a38437bfaa6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6fe127e29442e6a7b4fce25bf247b2",
            "placeholder": "​",
            "style": "IPY_MODEL_f9857b4917ce47988d3b5c3a2cada072",
            "value": "modules.json: 100%"
          }
        },
        "765cb7f89006430583975a2fbd2bf8a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7943d5371eac425b910def5df2e33b13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d831b79d171468ebbaea831f6d90e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8114d70d240e4badae3b1a868917ce96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fad3fdef714fd684ebedd91f283e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d64f76e0d95452d9440d79ff31fe469",
            "placeholder": "​",
            "style": "IPY_MODEL_885b8ad6c8ae448da2116b5ea9e6d52e",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "8492dba614e6424193605dba29a78f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86c2221e3bf94bd4a89591a4ccb36bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8728d8fef01f422489d209593bd16a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876a6a034a7348ae8c791fd728df4a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8728d8fef01f422489d209593bd16a61",
            "placeholder": "​",
            "style": "IPY_MODEL_4b6ef4d983244d2abb2c9c809b0fdbd2",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "884532546f864d82bcd8214c862001b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e142947baea4eb5b9ab9df1e77c1da1",
            "placeholder": "​",
            "style": "IPY_MODEL_72f9201df51c411da6a53ade73b5ae30",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "885b8ad6c8ae448da2116b5ea9e6d52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88fd9075b189479e91d2a2a560838d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bc1c069b07543edb08a5b7b56e44b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c02a0ec9ad94e169d6d09f859b7425b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_876a6a034a7348ae8c791fd728df4a31",
              "IPY_MODEL_340eef53102b4e5babfa903300549e9f",
              "IPY_MODEL_ea691b76d7394327ac7b18e916b811b6"
            ],
            "layout": "IPY_MODEL_41ec9baf2afa4639953b76f46e9f16f5"
          }
        },
        "8c9535abe7c140bbb2691f5f1cf5475d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b30794bc50c4af8a992438681157606",
              "IPY_MODEL_59eaa8a4c08a4a06881f659e98c7e9ee",
              "IPY_MODEL_9ef6007da43a433c9b914f096e871e3e"
            ],
            "layout": "IPY_MODEL_7d831b79d171468ebbaea831f6d90e2e"
          }
        },
        "8e0792165eb940d5a60153ba57a112f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d006af4d8c4b59bc7395cbd56d3711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c30fe920324a40dbaa6387a7a3f3157c",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fa261e9273c499ebd6944849dab849a",
            "value": 53
          }
        },
        "9545977952d243c7a78adeab8e9c01d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05d86ca46c3741ebabcb599313025b66",
            "placeholder": "​",
            "style": "IPY_MODEL_88fd9075b189479e91d2a2a560838d3a",
            "value": "tokenizer.json: 100%"
          }
        },
        "970df4d75ac4412293cf3e8675cbe108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2fb484ad5b4f8899d28cc498876a69",
            "placeholder": "​",
            "style": "IPY_MODEL_1a5a642761744fe393fd7ff8dec00a7d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9acb21553880459daaf2d0d4cf5ebb8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28bf3746ba70469fba60bdd39104bebb",
            "placeholder": "​",
            "style": "IPY_MODEL_d572e981411c41859bd309f1969cc10b",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 258MB/s]"
          }
        },
        "9c2f47a11da941eba186379dcfe89ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d2fb484ad5b4f8899d28cc498876a69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef6007da43a433c9b914f096e871e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3c04dfa1834e8e9a1c7e25e9de0c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c5750ecd88af429daa56049369ce583f",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 675kB/s]"
          }
        },
        "9f5cbf8067a14c3b971683ba2d34caa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f69bbcf1a364fbc9f5a4b87a9707e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b1ee232c7f4eebb0b8ee91017677b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a746fb03a0cc4beeb6963e529790bc9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85c8f3c6367410fbccfc422c5c94840": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d080a1849424cfe9b9eecf4cfd4177b",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86c2221e3bf94bd4a89591a4ccb36bf0",
            "value": 350
          }
        },
        "a9a32eb87eb041c98bc8797999a2a237": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abf71bfa63f84f31b9c91e3c81cfa095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9545977952d243c7a78adeab8e9c01d6",
              "IPY_MODEL_fa8e9e83ca4e4bcc9b2ea0556b8b9876",
              "IPY_MODEL_3744a5bc5d384c7980dc9062682d9cc4"
            ],
            "layout": "IPY_MODEL_8114d70d240e4badae3b1a868917ce96"
          }
        },
        "ac21399e28ad4d9e9b5efea14995a5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c90d06e5c57941bf8f77ebbebbbcbc1f",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f017f5f43a246a3bc532c2a52565daa",
            "value": 349
          }
        },
        "b930cbfca7f74078b93d5602773d296b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22629bfb6cec47ea8c23a506e7b4abed",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f637db2c6784169b3a9089e422d9c68",
            "value": 231508
          }
        },
        "ba169ce061744348be80bfbcb51ff43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f137ac74e5415b8f6948a979d640d1",
            "placeholder": "​",
            "style": "IPY_MODEL_415ae90ba08b4734ab80447e1c340ee7",
            "value": " 350/350 [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "bb63070499754237a8ec9cfaabf2c8af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb644df6e0aa44de864a9b27f5a7d647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0dad828e7c4d39904ee4441d6b4a70",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b04687b65d74da8bd11810627439bbc",
            "value": 190
          }
        },
        "bee80aed71fb40aa8ed13a832cf35d01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6fe127e29442e6a7b4fce25bf247b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01fc88f61ef4b63be7d56386df0852a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2406b587f5342ae9992ad077e6ddbe6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30fe920324a40dbaa6387a7a3f3157c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5750ecd88af429daa56049369ce583f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c671b5e4126c48428ff67ba13ee6f609": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bee80aed71fb40aa8ed13a832cf35d01",
            "placeholder": "​",
            "style": "IPY_MODEL_050d6aeca905427e960963c2b0be16e0",
            "value": " 190/190 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "c90448ad471343c78d39f96059837750": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90d06e5c57941bf8f77ebbebbbcbc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceab4691bb92485fb54e6ea10ff6a508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d10366be3405407bb593ec89157387ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d572e981411c41859bd309f1969cc10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da5f5463470544a38c61402ad6d45761": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db163de0d0404d45820a32a1a4b020aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f64e9893f4344cda9376e6665b9e9cc8",
            "placeholder": "​",
            "style": "IPY_MODEL_1a3455264b064db5956bf1bcdce49f88",
            "value": " 612/612 [00:00&lt;00:00, 37.9kB/s]"
          }
        },
        "dcfbb9f56c224e7889b4154dca553bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0dad828e7c4d39904ee4441d6b4a70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df7edbac29cf4716b012f33f8ba14a85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d9bdffab034566a0912667c994b0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7edbac29cf4716b012f33f8ba14a85",
            "placeholder": "​",
            "style": "IPY_MODEL_337bf17dd41547d794aa35196bc26a48",
            "value": " 349/349 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "e3d434f4ab6b4ad9b405bd3bb5551aff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c6900712734f08a6b706628b2e91fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea26c73dd146447293f9bc842d78434b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83fad3fdef714fd684ebedd91f283e51",
              "IPY_MODEL_bb644df6e0aa44de864a9b27f5a7d647",
              "IPY_MODEL_c671b5e4126c48428ff67ba13ee6f609"
            ],
            "layout": "IPY_MODEL_a746fb03a0cc4beeb6963e529790bc9e"
          }
        },
        "ea691b76d7394327ac7b18e916b811b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d10366be3405407bb593ec89157387ea",
            "placeholder": "​",
            "style": "IPY_MODEL_ceab4691bb92485fb54e6ea10ff6a508",
            "value": " 116/116 [00:00&lt;00:00, 7.44kB/s]"
          }
        },
        "ec63ed22e44d4a46882532e3e450868a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eda9ec3506504f11810f8d3aea112821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede51521ec5d4d4387fd1bb9664e4765": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c20fd03694a42dc9838e030bfe91a1a",
              "IPY_MODEL_088cb6eec8bd43c7878f28ead8ec77cc",
              "IPY_MODEL_db163de0d0404d45820a32a1a4b020aa"
            ],
            "layout": "IPY_MODEL_5087706f90e3474f821f987a1674e49d"
          }
        },
        "f4ad77781efc4c32a4c0146c12cd4ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f64e9893f4344cda9376e6665b9e9cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f828d752b80a4998b9ef583ffd7c5d73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8feff483deb41f6ae560c1672dfc140": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b91fef71310492d83331f6a4b55c24a",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19a211dff68544409f155d40014d39e8",
            "value": 90868376
          }
        },
        "f9857b4917ce47988d3b5c3a2cada072": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa8e9e83ca4e4bcc9b2ea0556b8b9876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2829de85fe43689143a27e2332c814",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_050051cf9cfb442b9a50e514f9e596a0",
            "value": 466247
          }
        },
        "fbffa3afffe841bb96a650f2725a1d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_741ecc3b9cd14bfb84d1a38437bfaa6f",
              "IPY_MODEL_ac21399e28ad4d9e9b5efea14995a5fd",
              "IPY_MODEL_e0d9bdffab034566a0912667c994b0df"
            ],
            "layout": "IPY_MODEL_c2406b587f5342ae9992ad077e6ddbe6"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
